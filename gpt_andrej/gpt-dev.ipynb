{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-28 15:44:54--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2023-02-28 15:44:54 (21.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny shakespeare data i/o and basic encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('total chars:', vocab_size)\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [20 43 50 50 53  1 61 53 56 50 42]\n",
      "decoded: Hello worlkd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create a mapping of unique chars to indices\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda x: np.array([char_to_idx[ch] for ch in x])\n",
    "decode = lambda x: ''.join([idx_to_char[idx] for idx in x])\n",
    "\n",
    "print('encoded:', encode(\"Hello world\"))\n",
    "print('decoded:', decode(encode(\"Hello worlkd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "# Convert text to encoded tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854]) torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "# Train and valid split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([18]), Target: 47\n",
      "Input: F, Target: i\n",
      "Input: tensor([18, 47]), Target: 56\n",
      "Input: Fi, Target: r\n",
      "Input: tensor([18, 47, 56]), Target: 57\n",
      "Input: Fir, Target: s\n",
      "Input: tensor([18, 47, 56, 57]), Target: 58\n",
      "Input: Firs, Target: t\n",
      "Input: tensor([18, 47, 56, 57, 58]), Target: 1\n",
      "Input: First, Target:  \n",
      "Input: tensor([18, 47, 56, 57, 58,  1]), Target: 15\n",
      "Input: First , Target: C\n",
      "Input: tensor([18, 47, 56, 57, 58,  1, 15]), Target: 47\n",
      "Input: First C, Target: i\n",
      "Input: tensor([18, 47, 56, 57, 58,  1, 15, 47]), Target: 58\n",
      "Input: First Ci, Target: t\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Input: {context}, Target: {target}\")\n",
    "    print(f\"Input: {decode(context.numpy())}, Target: {decode(np.array([target.numpy()]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "Batch 0:\n",
      "Input: tensor([57]), Target: 1\n",
      "Input: s, Target:  \n",
      "Input: tensor([57,  1]), Target: 46\n",
      "Input: s , Target: h\n",
      "Input: tensor([57,  1, 46]), Target: 47\n",
      "Input: s h, Target: i\n",
      "Input: tensor([57,  1, 46, 47]), Target: 57\n",
      "Input: s hi, Target: s\n",
      "Input: tensor([57,  1, 46, 47, 57]), Target: 1\n",
      "Input: s his, Target:  \n",
      "Input: tensor([57,  1, 46, 47, 57,  1]), Target: 50\n",
      "Input: s his , Target: l\n",
      "Input: tensor([57,  1, 46, 47, 57,  1, 50]), Target: 53\n",
      "Input: s his l, Target: o\n",
      "Input: tensor([57,  1, 46, 47, 57,  1, 50, 53]), Target: 60\n",
      "Input: s his lo, Target: v\n",
      "Batch 1:\n",
      "Input: tensor([1]), Target: 58\n",
      "Input:  , Target: t\n",
      "Input: tensor([ 1, 58]), Target: 46\n",
      "Input:  t, Target: h\n",
      "Input: tensor([ 1, 58, 46]), Target: 43\n",
      "Input:  th, Target: e\n",
      "Input: tensor([ 1, 58, 46, 43]), Target: 56\n",
      "Input:  the, Target: r\n",
      "Input: tensor([ 1, 58, 46, 43, 56]), Target: 43\n",
      "Input:  ther, Target: e\n",
      "Input: tensor([ 1, 58, 46, 43, 56, 43]), Target: 1\n",
      "Input:  there, Target:  \n",
      "Input: tensor([ 1, 58, 46, 43, 56, 43,  1]), Target: 41\n",
      "Input:  there , Target: c\n",
      "Input: tensor([ 1, 58, 46, 43, 56, 43,  1, 41]), Target: 39\n",
      "Input:  there c, Target: a\n",
      "Batch 2:\n",
      "Input: tensor([17]), Target: 26\n",
      "Input: E, Target: N\n",
      "Input: tensor([17, 26]), Target: 15\n",
      "Input: EN, Target: C\n",
      "Input: tensor([17, 26, 15]), Target: 17\n",
      "Input: ENC, Target: E\n",
      "Input: tensor([17, 26, 15, 17]), Target: 10\n",
      "Input: ENCE, Target: :\n",
      "Input: tensor([17, 26, 15, 17, 10]), Target: 0\n",
      "Input: ENCE:, Target: \n",
      "\n",
      "Input: tensor([17, 26, 15, 17, 10,  0]), Target: 32\n",
      "Input: ENCE:\n",
      ", Target: T\n",
      "Input: tensor([17, 26, 15, 17, 10,  0, 32]), Target: 53\n",
      "Input: ENCE:\n",
      "T, Target: o\n",
      "Input: tensor([17, 26, 15, 17, 10,  0, 32, 53]), Target: 1\n",
      "Input: ENCE:\n",
      "To, Target:  \n",
      "Batch 3:\n",
      "Input: tensor([57]), Target: 58\n",
      "Input: s, Target: t\n",
      "Input: tensor([57, 58]), Target: 6\n",
      "Input: st, Target: ,\n",
      "Input: tensor([57, 58,  6]), Target: 1\n",
      "Input: st,, Target:  \n",
      "Input: tensor([57, 58,  6,  1]), Target: 61\n",
      "Input: st, , Target: w\n",
      "Input: tensor([57, 58,  6,  1, 61]), Target: 47\n",
      "Input: st, w, Target: i\n",
      "Input: tensor([57, 58,  6,  1, 61, 47]), Target: 58\n",
      "Input: st, wi, Target: t\n",
      "Input: tensor([57, 58,  6,  1, 61, 47, 58]), Target: 46\n",
      "Input: st, wit, Target: h\n",
      "Input: tensor([57, 58,  6,  1, 61, 47, 58, 46]), Target: 0\n",
      "Input: st, with, Target: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "x_batch, y_batch = get_batch('train')\n",
    "print(x_batch.shape, y_batch.shape)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    print(f\"Batch {b}:\")\n",
    "    for t in range(block_size):\n",
    "        context = x_batch[b,:t+1]\n",
    "        target = y_batch[b,t]\n",
    "        print(f\"Input: {context}, Target: {target}\")\n",
    "        print(f\"Input: {decode(context.numpy())}, Target: {decode(np.array([target.numpy()]))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very simple Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) tensor(4.5700, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embeddings(idx)\n",
    "        B, T, V = logits.shape\n",
    "        \n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        logits = logits.view(B*T, V)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)  # transpose to get B x T x V\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]  # take the last token\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out, loss = m(x_batch, y_batch)\n",
    "print(out.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rERSLA:hsAy leWZhgYYt?$-.iF-wOY,FWORwQhBq:TwYYiW3uGJxi'?KO&.,-VG-vnvOaYz?L&\n",
      "zbgLvcgcnCAKY.PuKDBH.nhu\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "idx = m.generate(idx, 100)\n",
    "print(decode(idx[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss 2.5169339179992676\n",
      "Step 1000, Loss 2.517500400543213\n",
      "Step 2000, Loss 2.554572343826294\n",
      "Step 3000, Loss 2.440171241760254\n",
      "Step 4000, Loss 2.46695613861084\n",
      "Step 5000, Loss 2.3237228393554688\n",
      "Step 6000, Loss 2.502161979675293\n",
      "Step 7000, Loss 2.526960611343384\n",
      "Step 8000, Loss 2.4958548545837402\n",
      "Step 9000, Loss 2.538480520248413\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "batch_size = 32\n",
    "for steps in range(epochs):\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    \n",
    "    logits, loss = m(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 1000 == 0:\n",
    "        print(f\"Step {steps}, Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ye s t; CLogamowriour; thestot Eavius.\n",
      "Mag a at fitous rdod bullo?\n",
      "BRD yomu;\n",
      "\n",
      "TO: he ere I t? end whal\n",
      "\n",
      "\n",
      "THes the ch t che Isponds d tot nghe.\n",
      "\n",
      "Maits 'swe, theste rdit BERDinocowous soupa LA b y l the.\n",
      "haden baithio cr; Hee:\n",
      "\n",
      "BENCl do-cisise?\n",
      "Sigine oud I wenghath's pet, heangho,\n",
      "\n",
      "Prcr stomolow'\n",
      "\n",
      "ARI k'ss, angrpetangond! ed\n",
      "Thot f'lor rther yof l I nd BNGLA:\n",
      "\n",
      "\n",
      "nthoulotsmo fordsonot fueth yo thenoour eaus poy ist fond oo?\n",
      "TUDONCI s my, nd pawim'Her il fifos RD ul inge, f in at uliss Be.\n",
      "\n",
      "IOK:\n",
      "\n",
      "An\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "idx = m.generate(idx, 500)\n",
    "print(decode(idx[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toy example\n",
    "torch.manual_seed(42)\n",
    "B, T, V = 4, 8, 2\n",
    "x = torch.randn(B, T, V)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1\n",
    "# Calculate x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros_like(x)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xbow[b,t] = torch.mean(x[b,:t+1], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.9269,  1.4873],\n",
       "         [ 0.9007, -2.1055],\n",
       "         [ 0.6784, -1.2345],\n",
       "         [-0.0431, -1.6047],\n",
       "         [-0.7521,  1.6487],\n",
       "         [-0.3925, -1.4036],\n",
       "         [-0.7279, -0.5594],\n",
       "         [-0.7688,  0.7624]]),\n",
       " tensor([[ 1.9269,  1.4873],\n",
       "         [ 1.4138, -0.3091],\n",
       "         [ 1.1687, -0.6176],\n",
       "         [ 0.8657, -0.8644],\n",
       "         [ 0.5422, -0.3617],\n",
       "         [ 0.3864, -0.5354],\n",
       "         [ 0.2272, -0.5388],\n",
       "         [ 0.1027, -0.3762]]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version: 2\n",
    "# Vectorize damnit!\n",
    "avg_weights = torch.tril(torch.ones(T,T))\n",
    "avg_weights = avg_weights / torch.sum(avg_weights, dim=1, keepdim=True)\n",
    "avg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = avg_weights @ x # (T,T) @ (B,T,V) -> (B,T,T) @ (B,T,V) -> (B,T,V)\n",
    "torch.allclose(xbow, xbow2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3\n",
    "# Using softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "avg_weights = torch.zeros_like(tril)\n",
    "avg_weights[tril == 0] = float('-inf')\n",
    "avg_weights = F.softmax(avg_weights, dim=1)\n",
    "avg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = avg_weights @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4: Self-attention\n",
    "B, T, V = 4, 8, 32\n",
    "x = torch.randn(B, T, V)\n",
    "\n",
    "# Single head attention\n",
    "head_size = 16\n",
    "query = nn.Linear(V, head_size, bias=False)\n",
    "key = nn.Linear(V, head_size, bias=False)\n",
    "value = nn.Linear(V, head_size, bias=False)\n",
    "k = key(x)  # (B,T,head_size)\n",
    "q = query(x) # (B,T,head_size)\n",
    "\n",
    "affinity = q @ k.transpose(-2, -1) / head_size**0.5  # (B,T,head_size) @ (B,head_size,T) -> (B,T,T)\n",
    "# ^ normalise by the size of the head to prevent the dot product from exploding and causing softmaxx to converge to argmax\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "affinity = affinity.masked_fill(tril == 0, float('-inf'))\n",
    "# affinity[tril == 0] = float('-inf')\n",
    "affinity = F.softmax(affinity, dim=1)\n",
    "\n",
    "v = value(x)  # (B,T,head_size)\n",
    "out = affinity @ v  # (B,T,T) @ (B,T,head_size) -> (B,T,head_size)\n",
    "# out = affinity @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1388, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1164, 0.1128, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1756, 0.1034, 0.1615, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1272, 0.1324, 0.1881, 0.2397, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0749, 0.2171, 0.1666, 0.1622, 0.3265, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0942, 0.1995, 0.1404, 0.2054, 0.1477, 0.1700, 0.0000, 0.0000],\n",
       "        [0.1727, 0.1011, 0.1793, 0.1778, 0.3046, 0.4363, 0.3622, 0.0000],\n",
       "        [0.1003, 0.1337, 0.1641, 0.2149, 0.2213, 0.3937, 0.6378, 1.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affinity[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4cb1bbe0d7b001440e71de13b62787723eb0190e78868aa84558b9a5256dc09a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
